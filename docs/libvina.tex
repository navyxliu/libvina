
%% bare_conf.tex
%% V1.3
%% 2007/01/11
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.7 or later) with an IEEE conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: IEEEtran.cls, IEEEtran_HOWTO.pdf, bare_adv.tex,
%%                    bare_conf.tex, bare_jrnl.tex, bare_jrnl_compsoc.tex
%%*************************************************************************

% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. IEEE's font choices can trigger bugs that do  ***
% *** not appear when using other class files.                            ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/




% Note that the a4paper option is mainly intended so that authors in
% countries using A4 can easily print to A4 and see how their papers will
% look in print - the typesetting of the document will not typically be
% affected with changes in paper size (but the bottom and side margins will).
% Use the testflow package mentioned above to verify correct handling of
% both paper sizes by the user's LaTeX system.
%
% Also note that the "draftcls" or "draftclsnofoot", not "draft", option
% should be used if it is desired that the figures are to be displayed in
% draft mode.
%
\documentclass[10pt, conference, compsocconf]{IEEEtran}
% Add the compsocconf option for Computer Society conferences.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/tex-archive/macros/latex/contrib/oberdiek/
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 4.0 (2003-05-27) and later if using hyperref.sty. cite.sty does
% not currently provide for hyperlinked citations.
% The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/cite/
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
   \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
   \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
   \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
   \DeclareGraphicsRule{*}{mps}{*}{}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
   \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
   \graphicspath{{./}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
   \DeclareGraphicsExtensions{.png, .pdf, .eps}
\fi


% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at: 
% http://www.ctan.org/tex-archive/macros/latex/required/graphics/
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found as epslatex.ps or
% epslatex.pdf at: http://www.ctan.org/tex-archive/info/
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage[cmex10]{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics. If using
% it, be sure to load this package with the cmex10 option to ensure that
% only type 1 fonts will utilized at all point sizes. Without this option,
% it is possible that some math symbols, particularly those within
% footnotes, will be rendered in bitmap form which will result in a
% document that can not be IEEE Xplore compliant!
%
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithms/
% There is also a support site at:
% http://algorithms.berlios.de/index.html
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


%\usepackage{mdwmath}
%\usepackage{mdwtab}
% Also highly recommended is Mark Wooding's extremely powerful MDW tools,
% especially mdwmath.sty and mdwtab.sty which are used to format equations
% and tables, respectively. The MDWtools set is already installed on most
% LaTeX systems. The lastest version and documentation is available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/mdwtools/


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.


%\usepackage{eqparbox}
% Also of notable interest is Scott Pakin's eqparbox package for creating
% (automatically sized) equal width boxes - aka "natural width parboxes".
% Available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/eqparbox/





% *** SUBFIGURE PACKAGES ***
%\usepackage[tight,footnotesize]{subfigure}
% subfigure.sty was written by Steven Douglas Cochran. This package makes it
% easy to put subfigures in your figures. e.g., "Figure 1a and 1b". For IEEE
% work, it is a good idea to load it with the tight package option to reduce
% the amount of white space around the subfigures. subfigure.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at:
% http://www.ctan.org/tex-archive/obsolete/macros/latex/contrib/subfigure/
% subfigure.sty has been superceeded by subfig.sty.



%\usepackage[caption=false]{caption}
%\usepackage[font=footnotesize]{subfig}
% subfig.sty, also written by Steven Douglas Cochran, is the modern
% replacement for subfigure.sty. However, subfig.sty requires and
% automatically loads Axel Sommerfeldt's caption.sty which will override
% IEEEtran.cls handling of captions and this will result in nonIEEE style
% figure/table captions. To prevent this problem, be sure and preload
% caption.sty with its "caption=false" package option. This is will preserve
% IEEEtran.cls handing of captions. Version 1.3 (2005/06/28) and later 
% (recommended due to many improvements over 1.2) of subfig.sty supports
% the caption=false option directly:
%\usepackage[caption=false,font=footnotesize]{subfig}
%
% The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/subfig/
% The latest version and documentation of caption.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/caption/




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure. The latest version and documentation can be found at:
% http://www.ctan.org/tex-archive/macros/latex/base/



%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/sttools/
% Documentation is contained in the stfloats.sty comments as well as in the
% presfull.pdf file. Do not use the stfloats baselinefloat ability as IEEE
% does not allow \baselineskip to stretch. Authors submitting work to the
% IEEE should note that IEEE rarely uses double column equations and
% that authors should try to avoid such use. Do not be tempted to use the
% cuted.sty or midfloat.sty packages (also by Sigitas Tolusis) as IEEE does
% not format its papers in such ways.





% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/misc/
% Read the url.sty source comments for usage information. Basically,
% \url{my_url_here}.





% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{A Template Approach to Transform Programs in Static}


% author names and affiliations
% use a multiple column layout for up to two different
% affiliations

%\author{\IEEEauthorblockN{Authors Name/s per 1st Affiliation (Author)}
%\IEEEauthorblockA{line 1 (of Affiliation): dept. name of organization\\
%line 2: name of organization, acronyms acceptable\\
%line 3: City, Country\\
%line 4: Email: name@xyz.com}
%\and
%\IEEEauthorblockN{Authors Name/s per 2nd Affiliation (Author)}
%\IEEEauthorblockA{line 1 (of Affiliation): dept. name of organization\\
%line 2: name of organization, acronyms acceptable\\
%line 3: City, Country\\
%line 4: Email: name@xyz.com}
%}

% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
% 
\author{
\IEEEauthorblockN{Xin Liu$^\dag$, Daqiang Zhang$^\dag$, Jingyu Zhou$^\dag$, Minyi Guo$^\dag$}
\IEEEauthorblockA{Department of Computer Science\\
Shanghai Jiao Tong University\\
Dongchuan 800, Shanghai, P.R.China\\
\{navyliu, zhangdq\}@sjtu.edu.cn, \{guo-my, zhou-jy\}@cs.sjtu.edu.cn}
}

% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle


\begin{abstract}
In advent of multicore era, plain C/C++ programming language can not fully reflects the hardware architecture any more. A source-to-source compilation assists in adapting programs close to contemporary hardwares. We proposed a template-based approach to perform the transformation for programs with rich static information. We presented template meta-programming to conduct parallelization and memory hierarchical optimization. It enables programmers to adapt new architectural feature or parallel computation models by extending template library. 
In this paper, we implemented a prototype template library -- libvina to demonstrate the idea. Finally, We evaluate the performance on commodity x86 and GPU platforms by a variety of typical applications in multimedia and scientific fields. The experiments show that our approach is flexible to support multiple computational models. In addition, the experimental results reveal that our approach incurs little run-time overhead because it takes effects in compile-time.
\end{abstract}

\begin{IEEEkeywords}
static analysis; Compiler optimization; parallelization
\end{IEEEkeywords}


% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle

\section{Introduction}
In multicore period, hardware architects rely on parallelism and memory hierarchy to enhance performance. Both cloned processors and elaborated storage-on-chip require programmer to restructure their source code and keep tuning binaries for a specific target. Therefore, writing a high-performance application requires non-trivial knowledge of underlying machine's architecture. The gap between hardware vendors and software developers extends development cycle, which increases marketing cost and risks for innovative multicore architectures in silicon industry.

Algorithm experts usually focus on their specific domains and have limited insights on diverging computer systems. They expect hardware and optimized compiler to guarantee decent performance for their programs. The expectation was roughly held until parallel system was introduced to computer community. Since frequency of general purpose processor stops growing faster, it has been hard to obtain free performance enhancement from hardware's refinement any more. Essentially, plain C/C++ can not fully reflect contemplate architecture such as multicore and distributed storage-on-chip. Researchers have admitted that it is tremendously challenging for optimizing sequential code by a compiler, so the answer to bridge programmer to parallel hardware relies on language and library to express concurrency richer than ever.

Designing new parallel programming language is possible. Many functional languages \cite{b13} with inherent concurrency supports have emerged to the horizon of computer. However, a conservative programmer may turn them down because there are still lack of convincing evidences to demonstrate programmability and efficiency comparing to traditional programming languages. Another reason is software cost. Considering the time span which a large computer system serves, hardwares are cheap and become cheaper with time passing; software and well-trained personnel are expensive. Even numerous legacy systems designed with little consideration of multi-threaded environment at that time, vendors usually prefer to maintain and update legacy software systems for current and future hardwares rather than rebuilding them from scratch. 

One side, software developers insist on classic programming diagrams and are reluctant to rewrite existing sources. On the other side, exploiting horsepower of modern processors for existing and new systems is a moderate issue. Therefore, it is desirable to extend traditional programming languages to balance the trade-off.

Many programming languages extending C/C++ had been proposed for multicore. However, most of existing solutions aimed at specific architectures or computation models. UPC and OpenMP are designed for shared memory system \cite{b15}. CUDA works on vender-dependent GPU architecture for streaming computation; Sequoia \cite{b1} is an attempt to customize code-generation rules by XML configuration, however, it follows the similar restriction by enforce programs execute on a tree of abstract machines.

Our approach performs source-to-source transformation by compiler like Sequoia and OpenMP. We shared the same idea of  ~\cite{b1} to generate a cluster of subprocedures for a task recursively. Instead of modifying compiler and introducing new language constructs, we exploit the capability of C++ template mechanism to achieve translation. All transformation rules are programmed in C++ meta-programming \cite{b10} and are conducted by a group of template classes when that are instantiated. The primary limitation is that only type and static constant value are available in compile time. Therefore, it only works for programs which own rich static information. Fortunately, applications with this characteristic are pervasive in multimedia, digital processing, and scientific computation.

Because template takes effect in compile time, it is possible to avoid deploying runtime system, which means that it can incur minimal runtime cost. Besides that, our template-based approach imposes few restricts comparing with other static approaches:

\begin{itemize}
\item Meta-programming: We proposed a way to transform source code by
meta-programming. Because it can manipulate source code in compile
time,  our approach does not bind any execution model. It is easy to
change code to Fork-Join like \emph{OpenMP}, or perform computation
as pipeline. Furthermore, we can use all kind of thread
implementations, including pthread, native LWP provided by OS, even hardware thread.

\item Extendability:  It is flexible to develop new template class to utilize new architectural features. Template meta-programming is intimate for C++ programmers. It is easy to extend new execution models and parallel patterns. Other approaches have to ratify languages and then modify compiler to complete features.

\item Portability: Template mechanism are standardized in ISO
  \cite{b8, b17}. Template-based approach is applicable for every
  platforms with standard C++ supports. Except for platform features,
  template libraries are portable .
\end{itemize}

%organization structure
The remaining parts of this paper are structured as follows.
 Section 2 show techniques to perform
transforms by template meta-programming. Audiences with C++ template
programming experiences or functional programming concepts are helpful
but not prerequisites. Then Section 3 presents some typical transforms
performed by our approachs. Experiments are in Section 4 to evaluate
performance on both CPU and GPU respectively.
 Section 4 summarizes some related works on static transformation and other
library-based solutions.  Final section is conclusion and future works.

\section{Template Metaprogramming Approach}
\subsection{Overview}
We implemented a template library -- libvina, to perform source
transformation. One foundation of our approach is that assume C++
compiler front-end as a code generator. It actually practices
source-to-source transformation in the guidance of template
meta-programming. As mentioned before, template approach is limited in
compile time, therefore, problems we intend to solve must have rich static
information. Fortunately,  applications with this characteristic are
not uncommon in multimedia or digital processing fields, some of them 
even attract developers to implement them in hardware such as ASIC,
FPGA, or DSP. To leverage
static information, we provides data structures with
template parameters to carry such information. 

%When template classesare instantiated , compiler recursively
%generates codes until specific conditions are statisfied. 

In our design philosophy, we separate two roles in software
development. Domain-specific experts only care about writing efficient
algorithms in conventional C/C++ form. They provide interfaces in
forms of wrapper classes of functions. On the other side, system-level engineers develop C++ templates to take responsibility for
parallelization and memory optimization while keeping function's
type same. Optimizations for specific architectues are
achieved by applying a series of template classes. Porting to other platforms might need to refine
template parameters or apply to new templates, however, programmers do
not need to dig into algorithms again.

\subsection{Background}
%template general
Originally, C++ template mechanism is invented to supersede C preprocessor. It is type-safe and could facility generic programming. People found the potential of template computation by chance. \cite{b6} later proved template itself is Turing completeness. Beside the job it meant to do, template has successfully applied to many innovative purposes in modern C++ programming practices \cite{b9}. 

%template sepcialization
A powerful feature of C++'s templates is \emph{template specialization}. This allows alternative implementations to be provided based on certain characteristics of the parameterized type that is being instantiated. Template specialization has two purposes: to allow certain forms of optimization, and to reduce code bloat ~\cite{b18}.

%template metaprogramming 
\emph{Template meta-programming} is similar to functional programming language except it takes effect in compile time. It only relies on static information to determine control flow and perform computation. MPL Library \cite{b16} provides control statement and STL-like data structures, which greatly eases programming in static realm.

\subsection{Concepts}
\subsubsection{TF class}
%TF class
Computation-intensive functions are commonly referred to as
\emph{kernel} or \emph{filter}. Mathematically, a function is
single-target binary relation. Kernel functions are usually
self-contained, \textit{i.e.} external data references are limited and
calling graphs are simple. It's possible for a kernel function to
decouple into a group of subprocedures. Each subprocedure may be
exactly the same as kernel and spread on multicore running in
parallel.  Another approach is to divide a kernel into finer stages
and run in pipeline manner to respect data locality and bandwidth. In
libvina, a \emph{transform class (TF class)} is a template class which
transforms a function to a cluster of subprocedures in isomorphism. As
shown in Fig.~\ref{fig:tfcls}, the transformed function on right side
has the same interface while owns a call graph to complete the
original computation by a cluster of subprocedures. Execution of the
call graph can be programmed by in the library to take advantage of
architectural features.

\begin{figure}
\centering
\includegraphics[width=3.1in]{map-class}
\caption{transform class}\label{fig:tfcls}
\end{figure}

\subsubsection{Function Interface}\label{section:interface}
In programming language, a function which can applies any values of
different types are parametric polymorphism. C++ has already supported
this language feature by template function. Our library need to
manipulate template functions and instantiate them on demand, which we
call it \emph{late-instantiation} inspired of
\emph{late-binding}. Because the entry address of a template function is not available until instantiation, it is desirable to extend function
polymorphism for different template argumenta  at compile-time. Our
approach is to wrap the template function by a template class and pass
it as \emph{template template class}. A wrapper function acts
as \emph{interface} provided by algorithm developers. Fig.~\ref{lst:wrapper} is a wrapper function of vector addition. 

Beside it enables late-instantiation, the advantage of template class interface
is to provide different entries for different execution environments. \emph{doit\_b} is
used to implement synchronization in Fig.~\ref{fig:mm}.  Another benefit of
this form is that wrapper classes  give compiler a chance to select
appropriate codes based on their types, which is T in our example. This method incurs an extra function call  thought, it is hopefully  eliminated by
compiler's optimization.

\begin{figure}[!htp]
\begin{minipage}[tb]{\linewidth}
\makebox[\textwidth]{\hrulefill}
\begin{small}
\begin{verbatim}
template<class Result, class Arg0, 
         class Arg1>
struct vecAddWrapper {
 //...
 static void 
 doit(const Arg0& arg0, const Arg1& arg1, 
     Result& result)
 {
  vecArithImpl<T, DIM_N>::add(arg0, arg1, 
                           result);
 }
 static void 
 doit_b(const Arg0& arg0, const Arg1& arg1, 
     Result& result, mt::barrier& barrier)
 {
   doit(arg0, arg1, result);
   barrier.wait();
 }

 //...
};
\end{verbatim}
\end{small}
\makebox[\textwidth]{\hrulefill}
\end{minipage}
\caption{Function Wrapper}\label{lst:wrapper}
\end{figure}

%predicate and sentinel
\subsubsection{Predicate}
Borrowed from lisp concept, \emph{predicate} represents a indicator
of some conditions. It is a template class with static
fields initialized by constant expressions consisting of template
parameters and constants. These fields are automatically evaluated
when template classes are instantiated. Fig.~\ref{lst:pred} is an example to determine whether the problem size is fitting to last level cache.

\begin{figure}[!htp]
\begin{minipage}[tb]{\linewidth}
\makebox[\textwidth]{\hrulefill}
\begin{small}
\begin{verbatim}
template <class T, int SIZE_A
     , int SIZE_B, int SIZE_C>
struct p_lt_cache_ll {
 enum {CACHE_LL_SIZE = 4096*1024};
 const static bool value = 
     ((SIZE_A * SIZE_B 
   + SIZE_A * SIZE_C + SIZE_B * SIZE_C) 
   * sizeof(T) ) <= CACHE_LL_SIZE;
};
\end{verbatim}
\end{small}
\vspace{-1ex}\makebox[\textwidth]{\hrulefill}
\end{minipage}
\caption{Predicate}\label{lst:pred}
\end{figure}

\subsubsection{Sentinel}
\emph{Sentinels} in libvina are non-type template parameters of \emph{TF class}, with a \emph{predicate} as default initializer. When a template class is instantiating, sentinels are evaluated.  A \emph{predicate} determines whether a specific requirement has been satisfied. Sentinel is responsible for changing generation strategy according to the result. Using \emph{template specialization}, C++ compiler chooses different versions of a class to instantiate basing on the values or types of template arguments. The most important application of \emph{sentinel} is terminate code generation. More general flow control such as branch is feassible in ~\cite{b16}.

\subsection{Supporting data structure}
%view
Template meta-programing can only manipulate static information. As a
result, data structures need to carry such information as template
parameters. Only vector and matrix are implemented in our implementation,
because they cover many applications in multimedia
and scientific fields. Users require more versatile data structures could resort to
mature libraries such as \cite{b10}. 

\textbf{View} class is a concept to represent data set.  Fig. ~\ref{fig:view} depicts relationship of Views. Concrete lines
represent implicit conversion in C++, while  dashed lines are explicit
function calls to complete conversion. Text in edges are constraints
when conversions perform. Shadow region is another thread space. The
only approach to communicate with other thread is via ViewMT.  

Modern multicore architectures emphasize on utilization of 
bandwidth and storage-on-chip, therefore we manipulate data in
bulk way. Essentially, View is an abstract of \emph{stream} and it
helps programmers build streaming computation. Underneath of View
class, we can perform optimizations based on architectures. \textit{e.g}, it is not neccessary to duplicate
data  on shared memory, or  asynchronous communication can be raised to
hide latency. We define a signal class to synchronize, which mimic
signal primitive on CellBE ~\cite{b19}. It is implemented by
conditional variable on x86. For GPU, we use \emph{event} of OpenCL to
achieve the same semantics.

In addition,  View class is type-safed. Programmers can get
compilation errors if programs have potential violations of date access rules. Early errors are particularly
important to prevent programmer from trapping into multi-threaded bugs.

\begin{figure}
\includegraphics[width=3.3in]{view_concept}
\caption{View concept in libvina}
\label{fig:view}
\end{figure}
%%
\section{Source Transformations by Template}
\subsection{SPMD}
Multi-thread is dominant approach to utilize duplicated computational
units. SPMD model is the most intuitive thread model. Moreover,  SPMD is foundation of
streaming computation. There are numerous kernel functions in
multimedia applications and scientific computation which can 
exploit massive  data parallelism by dividing task into smaller and independent
subtasks. This characteristic can be naturally expressed by libvina's
TF class. We implement \emph{mappar} and \emph{mapreduce} language constructs in \cite{b1} as template classes.

Fig.~\ref{lst:mappar} is a definition of \emph{mappar} TF
class for x86. Template parameter \emph{Instance} is computation task
containing a kernel function wrapper and template arguments. The last template
parameter \emph{\_\_SENTINEL\_\_} determines control flow when
instantiation occurs. The second class in the figure is a template partial
specialization of prime template, which generates concrete thread
to practice computation. It is noteworthy that the last two arguments
are \emph{true}s, which means that this class is multi-threaded and
leaf node version. aux::subview is a \emph{meta}-function to cut off
Views. It could return a real subview or trivially return itself
according to types of template parameters. Fig. ~\ref{lst:mappar} gives the definition of arg1\_isomoph,
which determintes whether the \textit{arg1} is cutted off or keeps intact.

\begin{figure}[!htp]
\begin{minipage}[tb]{\linewidth}
\makebox[\textwidth]{\hrulefill}
\begin{small}
\begin{verbatim}
template <class Instance, int _K,
      bool _IsMT, bool __SENTINEL__ 
           = Instance::_pred>
struct mappar {
 typedef mappar<typename Instance::SubTask, 
       _K, _IsMT, 
       Instance::SubTask::_pred> _Tail;

 typedef typename mpl::or_<mpl::bool_
   <std::tr1::is_arithmetic
     <typename Instance::Arg1>::value>
   ,mpl::bool_<
     std::tr1::is_same<typename Instance::Arg1, 
              typename Instance::SubTask::Arg1>
      ::value>
   >::type
   arg1_isomorph;
 //...
 static void 
 doit(const typename Instance::Arg0& arg0, 
      const typename Instance::Arg1& arg1, 
        typename Instance::Result& result)
 {
   for (int k=0; k < _K; ++k) {
    auto subArg0 = ...
 
    auto subArg1 = aux::subview<decltype(arg0), 
        arg1_dim::value, arg1_isomorph::value>
            ::sub_reader(arg1, k); 
	  
    auto subResult = ...

    _Tail::doit(*subArg0,*subArg1,*subResult);
   }//end for
 }
};

template <class Instance, int  _K>
struct mappar <Instance, _K, true, true> 
{
 // ...
 static void 
 doit(const typename Instance::Arg0& arg0, 
   const typename Instance::Arg1& arg1,
   typename Instance::Result& result)
 {
   auto compF = Instance::computationMT();

   mt::thread_t leaf(compF, arg0, arg1, 
     __aux::ref(result, result_arithm()));
 }
};

\end{verbatim}
\end{small}
\vspace{-1ex}\makebox[\textwidth]{\hrulefill}
\end{minipage}
\caption{TF class of \textsl{mappar}}\label{lst:mappar}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=3.3in]{test_matrix}
\caption{MM internal call graph}\label{fig:mm}
\end{figure}

Fig.~\ref{fig:mm} is a cluster of subprocedures generated by libvina
after applying \emph{mapreduce} to a matrix multiplication
function. Template parameter \_K is 2 in this case because we intend
to perform this transform for a dual core machine. \emph{mapreduce}
divides a matrix into 4 sub-matrices. The figure except dashed lines
is actually a call graph. Shadow box is multi-threaded environment and two subprocedures are executed in parallel. Dashed lines indicate logical synchronization. 

\emph{mapseq} in \cite{b1} can be trivially implemented by passing false to \emph{\_IsMT} parameter. Nested block is possible by recursively defining \emph{TF classes}.

\subsection{Streaming and pipelining}
Streaming computation is a computer paradigm to perform massive parallelled computation. It models data set as a \emph{stream}. Operations are usually organized in pipeline way to process in turn, while keeping stream in on-chip storage. It can utilize multicore to perform computation in parallel and reduce external bandwidth. 

More general streaming computation does not restrict to keep data
stationary. Pipeline processing inherently support heterogeneous
architecture and ring network ~\cite{b19,b14}. If specific processors
are exposed by platform and communication cost is manageable, developers
intend to leverage  them for performance or energy
advantages. Template approach has no problem to link external
computational devices as long as developers provide communication layers.

Our template library provides two components to support streaming
computation. First, we provides multi-threaded \emph{ViewMT}
classes depicted in Fig.~\ref{fig:view}. Fig.~\ref{fig:viewmt}  is the
scenario in pipeline processing. We leverage ViewMTs to build a chain
among threads. Each stage takes charge of releasing the ownership of
data set after it finishes job.

Second, we provide a TF class to
build a pipeline. The simplified class is listed as follows
Fig.~\ref{lst:pipe}. The class chains a series of stages. It is noteworthy
that we dedicatedly the end of recursion because framework has no idea
how to deal with the output of pipeline. It is user's responsibility
to add final stage to clarify the behaviors. 
\begin{figure}[htp]
\includegraphics[width=3.1in]{viewmt}
\caption{ViewMT in pipeling}\label{fig:viewmt}
\end{figure}

\begin{figure}[!htp]
\begin{minipage}[tb]{\linewidth}
\makebox[\textwidth]{\hrulefill}
\begin{small}
\begin{verbatim}
template <typename... Stages>
struct pipeline;

template <class P, typename... Tail>
struct pipeline<P, Tail...> {
  typedef typename P::input_type in_t;
  typedef typename P::output_type out_t;
 
 static out_t doit(in_t in)
  {
    //... static checker
    pipeline<Tail...>::doit( P::doit(in) );
  }
};  
\end{verbatim}
\end{small}
\vspace{-1ex}\makebox[\textwidth]{\hrulefill}
\end{minipage}
\caption{TF class of pipeline}\label{lst:pipe}
\end{figure}

%\subsection{Blocking}
\subsection{Cooperation with other libraries}
We implemented our library in ISO standard
C++\cite{b8}. Theoretically, any standard-compliance C++ compiler
should process our classes without trouble. C++0X \cite{b17} added a
lot of language features to ease template meta-programming. Compilers
without C++0X supports need some workarounds to pass compilation
though, they do not hurt expressiveness. Consider the trend of C++, development of template library such as libvina should become easier and smoother in the future. 

We implemented \emph{mt::thread} based on underlying Linux PThread. A
simple C++ thread pool is developed to reduce cost of thread
creation. Because PThread does not have group-scheduling, we design
and implement a lightweight thread library(libSPMD) based on Linux
clone(2) and semaphore. For GPU,we use OpenCL to obtain platform
independence. Many accelerators are scheduled or declared to
implement OpenCL, which might extend our approach to new
territories in the future.  Although the interfaces of threads
memtioned before  are varying, our library can deal with them well.

%With the astonishing pace of core proliferation and architecture
%refinement, GPU has evolved to a pioneer of parallel computation,
%therefore, exploit of GPU bacomes urgent.

%OpenCL \cite{b12} is an industrial standard for heterogeous
%computation platforms.

%The prime downside of OpenCL is the obvious bias toward streaming
%architectures, which washes out many optimization techniques for
%traditonal architectures. Because our template
%approach is "meta'' programming techniqute,  it can mix OpenCL with
%other specialization methods. 

%Addionally, recursion is not supported by OpenCL in order to meet GPU's limitation.
\section{Experiment}
\subsection{Methodology}
C++0X has been partially supported by some mainstreaming compilers. Currently, we developed the library for gcc 4.4.0. GPU performance is measured on Mac OSX 10.6. 

A couple of algorithms are evaluated for our template approach.  They
are typical algorithms in image processing and scientific field. In
addition, we implemented a fake language translation scenario to
illustreate pipeline processing. The programs in expriments are listed
as follows:

\begin{itemize}
\item[saxpy] Procedure in BLAS level 1. A scalar multiplies to a single precision vector, which contains 32 million elements.
\item[sgemm] Procedure in BLAS level 3. Two 4096X4096 dense matrices multiply.
\item[dotprod] Two vectors perform dot production.
\item[conv2d] 2-Dimensional convolution operation on image.
\item[lang\_pipe] Pseudo-Multi-language translation. A word is translated from one language A to language B, and then new function will translate it from language B to language C, etc.
\end{itemize}

Two multicore platforms are used to conduct experiments. The hardsware
envronments are summaried in the following table:

\begin{table}[hb]
\caption{Experimental platforms}\label{tbl:mach}
\begin{center}
\begin{tabular}{|l|l|l|l|r|}
\hline
\textbf{name}&\textbf{type}&\textbf{processors}&\textbf{OS}\\
\hline
harpertown&SMP server&intel quad-core  2.0Ghz&Linux Fedora\\
                  &                &
                  & kernel 2.6.30\\
\hline
macbookpro&laptop &intel dual-core  2.63Ghz&Mac OSX\\
                     &           &nvidia 9400m 1.1Ghz     & Snowleopard\\
\hline
\end{tabular} 
\end{center}
\end{table}
On harptown, we link Intel Math kernel to perform BLAS procedures
except for conv2d. On macbookpro, we implemented the algorithms on our own.
\subsection{Results}

\begin{figure}
\includegraphics{speedupx86.0}
\caption{Speedup on Hapertown}\label{fig:spdx86}
\end{figure}

Fig.~\ref{fig:spdx86} shows the speedup on harpertown. HP Proliant
blade server contains 2-way 8 cores x86 Xeon
processors. We obversed good performance scalalbility for programs
sgemm and conv2d. The first one use \emph{mapreduce} pattern and the
other uses \emph{mappar}.  Both of them are close to upper-limited
speedup on target machines.  dotprod and saxpy reveal low speedup
because non-compute-intensive programs are subject to memory bandwidth. saxpy needs one load and one 
store for every two operations, and dotprod has similar
situation. They quickly saturate memory bandwidth for SMP system. 

\begin{figure}
\includegraphics{speedupgpu.0}
\caption{Speedup Comparing GPU with CPU}\label{fig:spdgpu}
\end{figure}

Fig.~\ref{fig:spdgpu} shows transformation results for GPU on
macbookpro. Programs run on host CPU  in sequence, and embedded GPU on motherboard contains 2 SMs\footnote{Streaming
  Multiprocessor, each SM consists of 8 scalar processors(SP)}
Porting from CPU to GPU, developer only need a couple of lines to change
templates and keep algorithm the same \footnote{
Because GPU code needs special qualifiers, we did decorate kernel
functions. We keep algorithms the same except for sgemm. It is not easy
 to work out sgemm for a laptop, we added blocking and SIMD
 instruments for CPU.} As figure depicted,  compute-intensive programs
sgemm and conv2d still maintain their speedups. 
In addtion, we observed about 2 times performance improvement comparing with saxpy counterpart on CPU. GPU can map
massive threads in group of warp(32 threads) on hardware and memory
accesses are possible to  be coalesed if a half of warps satisfy some  specific access patterns. Memory coalesce
mitigates bandwidth issue occurred on CPU. Because dotprod has fixed
step to access memory, we can not obtain hardware optimization
without tweaking the algorithm.

\begin{table}[hbt]
\caption{Comparison of sgemm on CPU and GPU}\label{tbl:sgemm}
\begin{tabular}{|l|r|r|r|}
\hline
& Seqential& CPU & GPU\\
\hline
\textbf{cores} &1  & 8 & 2 SMs\\
\hline
\textbf{Gflops}& 2.64 &95.6&  12.0\\
\hline
\textbf{utilization}&12.6\%& 74.9\%&68.2\%\\
\hline
\textbf{lines of function}&63&unknown&21\\
\hline
\end{tabular}
\end{table}

Table. ~\ref{tbl:sgemm} details sgemm execution on CPU and GPU. Dense maxtrix
multiplication is a typical computation-intensive program.
Our template transforms a sequential sgemm both for  CPU to embedded GPU.
We choose sequential execution on macbookpro's CPU as baseline. After
mapping the algorithm to GPU, we obtains over 4.5 times speedup
compare with host CPU. Intel core 2 microprocessor can issue 2 SSE instructions per
cycle,  therefore, the theoritical peak float performance is 21
Gflops. We obtain only 2.64 Gflops which utilization percentage is only
12.6\% even develop quite complicated source. On the otherside, 12
Gflops is observe on GPU whose theorically maximal
performance is roughly 17.6Gflops.\footnote{$17.6Gflops = 1.1Ghz * 2(SM) *
  8(SP)$. nVidia declares their GPUs can perform a mad(multiply-add
  op) per cycle  for
  users who concern performance over precision. However, we can
  not observe mad hints bring any performance improvement in OpenCL. }  The
implementation of sgemm on GPU is obviously easier than CPU counterpart though
the algorithm are almost the same. Applying \emph{mapreduce} template
for GPU only need tens of lines code effort. Like GPU template, we apply
\emph{mapreduce} for CPU, and then obverse 95.6 peak
Gflops with about 75\% utilization percentage on harpertown.

\section{Related work}
As mentioned before, it is desirable to extend conventional programming languages to reflects the essence of newer hardware. Researches in the field have two major directions:
\begin{enumerate}
\item providing new library to support programming in concurrency
\item extending language constructs to extend parallel semantics
\end{enumerate}

First, library is a common method to extend language capability
without introducing new language constructs or modifying
grammar. POSIX Thread library(\emph{pthread}) is a de facto standard for
multi-threading on UNIX-compatible systems. The relationship between pthread and native thread provided by OS is straightforward. Therefore, abstraction of pthread is far away from expressing parallelism and concurrency naturally. Furthermore, the implementation of thread on hardware is unspecific in the standard, so it can not guarantee performance or even correctness on some architectures \cite{b4, b5}.

C++ community intends to develop parallel library while bearing generic programming in mind. TBB(Thread Block Builder) has a plenty of containers and execution rules. Entities including partitioner and scheduler in TBB are created in runtime. In that case, key data structures have to be thread-safe. Although TBB exploits task parallelism or other sophisticated concurrency on general purpose processors, the runtime overhead is relative high in data parallel programs, especially in the scenario that many ALUs are exposed by hardware. Solution we proposed here is orthogonal to runtime parallel libraries. We only explore parallelism that can be determined in compile time, developers feel free to deploy other ways to speedup the programs in runtime, such as TBB.
 
%% MPI
%Another dominant parallel library is MPI in supercomputing community. It based on message-passing mechanism and SPMD model to execute parallel program. The difficulties of developing MPI programs are as notorious as pthread counterpart for programmers without sufficent training of parallel programming. 
%%

Second choice for language communities is to extend semantics by
modifying compiler. They add directive or annotation to help compiler
transform source code. \emph{OpenMP(OMP)} ~\cite{b20} is designed for shared memory and has shipped in almost every C/Fortran compilers. OMP compilers transform sequential code into multi-threaded equivalence with runtime. Although OpenMP is simple and portable, the performance is not optimal in most cases. A handful of directives leave little room for further enhancement or scaling up to larger systems. Hybrid OpenMP with MPI is possible, but difficulty surges.

Sequoia \cite{b1} is a source-to-source compiler to support
programming memory hierarchy. First of all, It targets execution
environment as a tree of machines, which an individual machine owns
its storage and computation unit. Second, It terms a
computation-intensive function as \textit{task}. New constructs apply
on \emph{task}. Sequoia compiler transforms a \textit{task} into a
cluster of subprocedures based on machines hierarchy. Target machine
is described in XML files. Based on this idea, Sequoia actually can specialize task for target architectures. \cite{b2} reports Sequoia can successfully transform codes for CellBE, SMP, cluster while keeping very competitive performance. However, Sequoia as a language extension is restrictive. The basic data structure is array, which obviously shows the preference of scientific computation. The language constructs do not cover all the common parallel patterns such as pipeline and task queue. Libvina derived the idea of Sequoia to transform source code recursively. However, template meta-programming utilizes existing mechanism in C++ compiler and is capable to express all the semantics in Sequoia programming language. Moreover, Sequoia can not leverage type system to specialize code. \textit{e.g.} Many modern processors usually provide SIMD instructions. Libvina could generate corresponding source based on predefined vector types. Sequoia compiler ignores this important information and simply relies on native compilers.

\section{Discussion and Future work}
%\newpage
We present a template-based approach to perform source-to-source
transoformation for programs with rich information. Because it applies 
metaprogramming technique, template approach does not bind to any execution
model. Therefore, we can synthesize more than one models a program. In
addion, our approach
is flexible and extensible. Instead of modifying a compiler to add
annotations or extend grammar, we implement the all functionalities by template mechanism. Template library is
intimate for C++ developers so they can extend the library to adapt
innovative architectures.  Our approach follows C++ standards,
which means that the methodology should work fine for every platform with
standard-compliant C++ compilers.  libvina is a prototype
implementation to demonstrate the idea in this paper.

Our programming model bridges algorithm experts and diverging multcore
architectures. Domain-specific experts focus on algorithms in form of
conventional programming languages. They wrap functions to template
classes and then pass them to \emph{TF class} as template parameter. Template
mechniasm takes responisbility to transform source code according to
their targets.

libvina is a prototype library, therefore some code tranformations
have not be implemented yet. Blocking is mature techniques to optimize
memory for single processor.  We think it is also feasible to apply
template  transformation for locality improvement.

Streaming is an important computation model for communication-exposed
parallel architectures. We paritially exploit the utilization of GPU in this
paper though,  it is still unclear how much efforts
should be pay to develop full-blown template library to support
streaming computation.  Existing implementation can only deal with regular
data. \emph{Gather-and-scatter} operation is not supported by our template
library. Futher work on libvina will concentrate on communication
abstract of streaming computation.

General applications also contain a variety of static information to
optimize even memory footprints are not regular. It is desirable
explores other execution models to utilize the key hint to transform
source code close to target architectures.

% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later

\IEEEtriggeratref{11}

% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,TCRef.bib}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)

\begin{thebibliography}{99}
\setlength{\itemsep}{1mm}
\bibitem{b1} Fatahalian, K., Knight, T., Houston, M., Erez, M., Horn, D.R., Leem, L., Park, J.Y., Ren, M., Aiken, A., Dally, W.J., Hanrahan, P.: Sequoia: Programming The Memory Hierarchy. SC2006
\bibitem{b2} Knight, T. J., Park, J. Y., Ren, M., Houston, M., Erez, M., Fatahalian, K., Aiken, A., Dally, W. J., and Hanrahan, P. Compilation for Explicitly Managed Memory Hierarchies. PPoPP 2007
\bibitem{b3} Aho, A., Sethi, R., Ullman, J.D., Lam, M.: Compilers: Principles, Techniques, and Tools. Addison-Wesley. 
\bibitem{b4} Boehm, H.J.: Threads Cannot Be Implemented As a Library. PLDI '05
\bibitem{b5} Drepper, U., Molnar, I.: The Native POSIX Thread Library for Linux. 2005
\bibitem{b6} Veldhuizen, T. L.: C++ Template are Turing Complete. 
\bibitem{b7} Saha, B., Zhou, X., Chen, H., Gao, Y., Yan, S., Rajagopalan, M., Fang, J., Zhang, P., Ronen, R., Mendelson, A.: Programming Model for Heterogeneous x86 Platform. PLDI '09.
\bibitem{b8} C++ Standard Committee: ISO/IEC 14882:2003(E) Programming Languages  C++, 2003
\bibitem{b9} Alexandrescu, A.: Modern C++ design: generic programming and design patterns applied, 2001
\bibitem{b10} Abrahams, D., Gurtovoy, A.: C++ Template Meta-programming: Concepts, Tools, and Techniques from Boost and Beyond, 2004
\bibitem{b11} Kapasi, U.J., Dally, W.J., Rixner, S., Owens, J.D., Khailany, B.: The Imagine Stream Processor. Proceeding of the 2002 International Conference on Computer Design.
\bibitem{b12} Munshi, A., Khronos OpenCL Working Group, The OpenCL Specification ver.1.0.43, 2009
\bibitem{b13} Goldberg, B.: Functional Programming Language, ACM Computing Surveys, 1996
\bibitem{b14} Seiler, L., Carmean, D., Sprangle, E., Forsyth, T., Abrash, M., Dubey, P., Junkins, S., Lake, A., Sugerman, J., Cavin, R., Espasa, R., Grochowski, E., Juan, T., Hanrahan, P. 2008. Larrabee: A ManyCore x86 Architecture for Visual Computing. ACM Trans. Graph. 27, 3, Article 18 (August 2008), 15 pages. DOI = 10.1145/1360612.1360617 http://doi.acm.org/10.1145/1360612.1360617. 
\bibitem{b15} El-Ghazawi, T., Cantonnet, F., Yao, Y, Rajamony, R.: Developing an Optimized UPC Compiler for Future Architecture
\bibitem{b16} Gurtovoy, A., Abrahams, D.: The BOOST C++ Meta-programming Library
\bibitem{b17} C++ Standard Committee: ISO/IEC DTR 19768 Doc No. 2857, 2009
\bibitem{b18} Stroustrup, B.: The C++ Programming Language (Spcial Edition) Addison Wesley. Reading Mass. USA. 2000. ISBN 0-201-70073-5
\bibitem{b19} J. A. Kahle, M. N. Day, H. P. Hofstee, C. R. Johns, T. R. Maeurer, and D. Shippy: Introduction to the Cell Multiprocessor, IBM Journal of Research and Development 49, No. 4/5, 589-604, 2005
\bibitem{b20} Official OpenMP Specifications, OpenMP Architecture
  Review Board, 2002, http://www.openmp.org/specs/.
\bibitem{b21}Linderman, M. D., Collins, J. D., Wang, H., Meng, T. H.:
  Merge: A Programming Model for Heteogeneous Multi-core Systems. ASPLOS '08, March 1-5, Seattle, USA
\end{thebibliography}




% that's all folks
\end{document}


